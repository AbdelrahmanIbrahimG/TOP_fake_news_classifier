{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WFwa7M7xrsw"
   },
   "source": [
    "# Importing the libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AoMFY-4RxxKP",
    "outputId": "917c8b82-aa70-4bbb-aaf3-847606ac4a00"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, ComplementNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "import pickle\n",
    "\n",
    "# !pip install contractions\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKwx-2awzIzJ"
   },
   "source": [
    "# Defined methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzUZN53izM9o"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # \\S matches any character that is not a space tab newline\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    #remove emojis\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    # igore => specifies how to handle characters that cannot be represented in ASCII\n",
    "    # remove htmk tags  => . any char except newline\n",
    "    text = re.sub(r'<.*>', '', text)\n",
    "    # remove punctiuations\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        if word.lower() not in english_stopwords:\n",
    "            filtered_words.append(word)\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text\n",
    "\n",
    "def spell_check_and_correction(text):\n",
    "    blob = TextBlob(text)\n",
    "    corrected_text = str(blob.correct())\n",
    "    return corrected_text\n",
    "\n",
    "def to_lower(s):\n",
    "    return s.lower()\n",
    "\n",
    "def expanding_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def lemmatization(text):\n",
    "    text_tokenized = []\n",
    "    \n",
    "    words = nltk.word_tokenize(text) #Tokenize\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for word in words:\n",
    "        text_tokenized.append(lemmatizer.lemmatize(word)) #lemmatize\n",
    "    \n",
    "    text_tokenized = ' '.join(text_tokenized)\n",
    "    \n",
    "    return text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDf9tZIZ_mWy",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = clean_text(text)\n",
    "#     text = spell_check_and_correction(text)\n",
    "    text = expanding_contractions(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = to_lower(text)\n",
    "    text = lemmatization(text)\n",
    "    return text\n",
    "\n",
    "# Testing the methods\n",
    "# print(preprocess_text(\"print they should pay all the back all the money plus interest the entire family and everyone who came in with them need to be deported asap why didn't it take two years to bust them here we go again another group stealing from the government and taxpayers a group of somalis stole over four million in government benefits over just  months weve reported on numerous cases like this one where the muslim refugeesimmigrants commit fraud by scamming our systemits way out of control more related\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "jeCPjdzYyYfy",
    "outputId": "b4c76d63-386b-4433-c83e-63ab88865fe2"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"..\\..\\dataset.csv\", encoding = \"latin1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GvH5D7O2TUqc",
    "outputId": "eb48a68d-5754-45ab-9968-14c746d4e60c"
   },
   "outputs": [],
   "source": [
    "# specify a small information about the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BFNFgS9YTP18",
    "outputId": "50fd6438-e8e0-4fd1-9b62-6eca1c196a5f"
   },
   "outputs": [],
   "source": [
    "# display the number of each category with its data type\n",
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptDzMYALTSzk",
    "outputId": "54f0c921-08ce-4257-b912-4d55aa627ec5"
   },
   "outputs": [],
   "source": [
    "# display the number of the nan values in each column\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the number of the duplicated rows\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the minimum and maximum number of words in a row of the dataset\n",
    "word_count = df['text'].str.split().str.len()\n",
    "print(f\"The minimum number of words: {int(word_count.min())}\")\n",
    "print(f\"The maximum number of words: {int(word_count.max())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKFisU-OyP2Q"
   },
   "source": [
    "# Data preprocssing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nan and -inf values\n",
    "df = df.dropna()\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the type of the label column to int\n",
    "df['label'] = df['label'].astype(int)\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the duplicates\n",
    "df = df.drop_duplicates()\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the rows\n",
    "df = df.sample(frac=1)\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMbR2Z6JTfFU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(preprocess_text)\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the rows with less than 20 words\n",
    "word_count = df['text'].str.split().str.len()\n",
    "print(f\"The minimum number of words before filtering: {int(word_count.min())}\")\n",
    "df = df[df['text'].str.split().str.len().gt(99) & df['text'].str.split().str.len().lt(581)]\n",
    "word_count = df['text'].str.split().str.len()\n",
    "print(f\"The minimum number of words before filtering: {int(word_count.min())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.boxplot(df['text'].str.split().str.len(), labels=[\"Series 1\"], vert=False, patch_artist=True)\n",
    "plt.xlabel(\"Series Name\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Boxplot of Series Data\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability (optional)\n",
    "plt.grid(True)  # Add grid lines (optional)\n",
    "# Customize box appearance (optional)\n",
    "boxes = plt.gca().get_children()[0]  # Get boxplot artist\n",
    "boxes.set_facecolor('lightblue')  # Set box color\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "# Count the occurrences of 0 and 1 in the 'label' column\n",
    "label_counts = df['label'].value_counts()\n",
    "# Extract the actual values (0 and 1) as a list\n",
    "label_values = label_counts.index.to_list()\n",
    "# Extract the counts (occurrences) as a list\n",
    "value_counts = label_counts.to_list()\n",
    "# Create the pie chart\n",
    "fig = px.pie(values=value_counts, names=label_values)\n",
    "fig.update_traces(hoverinfo='label+percent',\n",
    "                  textinfo='percent', \n",
    "                  textfont_size=20,\n",
    "                  marker=dict(colors=['gold', 'mediumturquoise'], \n",
    "                              line=dict(color='#000000', width=2)))\n",
    "fig.update_layout(\n",
    "    title_text=\"Label column pie chart\",\n",
    "    title_font_color=\"white\",\n",
    "    paper_bgcolor=\"black\",\n",
    "    font_color=\"white\") \n",
    "# Add data labels with percentages (optional)\n",
    "# fig.update_traces(textposition='inside', textinfo='percent+n')  # Adjust position and content\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"..\\..\\df_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"..\\..\\df_processed.csv\")\n",
    "# df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6612036)\t0.0619587354084424\n",
      "  (0, 6611593)\t0.06429091619755256\n",
      "  (0, 6611432)\t0.04724602950799684\n",
      "  (0, 6585801)\t0.02574934413884835\n",
      "  (0, 6585176)\t0.011437539463616937\n",
      "  (0, 6529870)\t0.06649099809187176\n",
      "  (0, 6528747)\t0.06795006078307725\n",
      "  (0, 6525786)\t0.031018958332892397\n",
      "  (0, 6402655)\t0.06649099809187176\n",
      "  (0, 6400758)\t0.025813994387668692\n",
      "  (0, 6211160)\t0.05511065908561874\n",
      "  (0, 6210215)\t0.023115318086619978\n",
      "  (0, 6159496)\t0.02875546972828243\n",
      "  (0, 6155127)\t0.013609568528639909\n",
      "  (0, 6053597)\t0.07701458614993596\n",
      "  (0, 6053560)\t0.05759201758990124\n",
      "  (0, 5985533)\t0.07701458614993596\n",
      "  (0, 5981079)\t0.043281211088827556\n",
      "  (0, 5977559)\t0.023006644621119527\n",
      "  (0, 5966832)\t0.06717991604005859\n",
      "  (0, 5964733)\t0.019127101425529987\n",
      "  (0, 5946362)\t0.06264765335662922\n",
      "  (0, 5945633)\t0.024172464334061027\n",
      "  (0, 5828282)\t0.07701458614993596\n",
      "  (0, 5828237)\t0.042671688236287725\n",
      "  :\t:\n",
      "  (95938, 320111)\t0.0264313033197721\n",
      "  (95938, 308328)\t0.04406000709609513\n",
      "  (95938, 308024)\t0.06185413564258758\n",
      "  (95938, 307901)\t0.04993852701795768\n",
      "  (95938, 276183)\t0.08282822960746723\n",
      "  (95938, 276160)\t0.054164250861120894\n",
      "  (95938, 275287)\t0.06524180698700216\n",
      "  (95938, 259268)\t0.057351784875467715\n",
      "  (95938, 259007)\t0.03392746392166803\n",
      "  (95938, 258884)\t0.04840324557106514\n",
      "  (95938, 258738)\t0.04460164881808044\n",
      "  (95938, 258491)\t0.034594222092520714\n",
      "  (95938, 257851)\t0.09011265407951917\n",
      "  (95938, 241207)\t0.024382528590771443\n",
      "  (95938, 238731)\t0.012859867827161163\n",
      "  (95938, 219837)\t0.054164250861120894\n",
      "  (95938, 219813)\t0.03172745715851994\n",
      "  (95938, 211501)\t0.052299662992954826\n",
      "  (95938, 211209)\t0.021472236808200457\n",
      "  (95938, 199632)\t0.04219541922792907\n",
      "  (95938, 198220)\t0.013907747055300948\n",
      "  (95938, 174512)\t0.03974524326018393\n",
      "  (95938, 171687)\t0.014404698051260259\n",
      "  (95938, 155017)\t0.04406000709609513\n",
      "  (95938, 154964)\t0.03192393377734573\n"
     ]
    }
   ],
   "source": [
    "# vect = TfidfVectorizer()\n",
    "# tfidf = vect.fit_transform(df['text'].values)\n",
    "# print(tfidf)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "counts = count_vectorizer.fit_transform(df['text'].values)\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting to training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(tfidf, df['label'], test_size=0.2 , shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model , name):\n",
    "    model.fit(x_train,y_train)\n",
    "    print(f\"Training accuracy of {name} is {round(model.score(x_train, y_train) * 100, 3)}%\")\n",
    "    print(f\"Testing accuracy of {name} is {round(model.score(x_test, y_test) * 100, 3)}%\")\n",
    "    print()\n",
    "    return round(model.score(x_test, y_test) * 100, 3)\n",
    "    \n",
    "def conf_matrix(model):\n",
    "    ConfusionMatrixDisplay.from_estimator(model, x_test, y_test)\n",
    "    \n",
    "def class_report(model):\n",
    "    print(classification_report(y_test, model.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passive Aggresive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac = PassiveAggressiveClassifier(max_iter = 1000)\n",
    "pac_acc = train(pac, \"Passive Aggresive Classifier\")\n",
    "conf_matrix(pac)\n",
    "class_report(pac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr_acc = train(lr, \"Logistic Regression\")\n",
    "conf_matrix(lr)\n",
    "class_report(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(loss='modified_huber', alpha=0.0001, max_iter=1000)\n",
    "sgd_acc = train(sgd, \"Stochastic Gradient Descent\")\n",
    "conf_matrix(sgd)\n",
    "class_report(sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(alpha=0.8, fit_prior=True, force_alpha=True)\n",
    "mnb_acc = train(mnb, \"Multinomial NB\")\n",
    "conf_matrix(mnb)\n",
    "class_report(mnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Complement NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnb = ComplementNB()\n",
    "cnb_acc = train(cnb, \"Complement NB\")\n",
    "conf_matrix(cnb)\n",
    "class_report(cnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt_acc = train(dt, \"Decision Tree\")\n",
    "conf_matrix(dt)\n",
    "class_report(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel=\"rbf\", gamma=0.1, C=0.1)\n",
    "svc_acc = train(svc, \"Support Vector Classification\")\n",
    "conf_matrix(svc)\n",
    "class_report(svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=5, random_state=42)\n",
    "rf_acc = train(rf, \"Random Forest\")\n",
    "conf_matrix(rf)\n",
    "class_report(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"Passive Aggressive\", \"Logistic Regression\", \"Decision Tree\",\n",
    "                   \"Random Forest\", \"SGD\", \"SVC\", \"Multinomial NB\", \"Complement NB\"]\n",
    "# dt_acc = 82.9\n",
    "# rf_acc = 84\n",
    "# svc_acc = 80\n",
    "\n",
    "# pac_acc = 81.5\n",
    "# lr_acc = 85.8\n",
    "# sgd_acc = 86.2\n",
    "# mnb_acc = 78.8\n",
    "# cnb_acc = 78.63\n",
    "accuracies = [pac_acc, lr_acc, dt_acc, rf_acc, sgd_acc, svc_acc, mnb_acc,\n",
    "                           cnb_acc]\n",
    "colors = [\"blue\", \"green\", \"orange\", \"yellow\", \"red\", \"violet\", \"black\", \"gold\", \"mediumturquoise\"]\n",
    "\n",
    "plt.figure(figsize=(12.5, 5))\n",
    "plt.bar(models, accuracies, color=colors)\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Model Accuracy Comparison\")\n",
    "for i in range(len(models)):\n",
    "        plt.text(i, accuracies[i], accuracies[i], ha = 'center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearly SGD has the hightest accuracy so we will save it for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the TF-IDF and model objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vect, open('vect.pkl', 'wb'))\n",
    "vect = pickle.load(open('vect.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lr, open('sgd.pkl', 'wb'))\n",
    "model = pickle.load(open('sgd.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifing a single news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_news(article):\n",
    "    article = preprocess_text(article)\n",
    "    article = [article]\n",
    "    tfidf = vect.transform(article)\n",
    "    prediction = model.predict(tfidf)\n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = fake_news(\"\"\"Andrea Tantaros, a former Fox News host, charged in a lawsuit filed Monday that top executives at the network, including the man who replaced Roger Ailes, punished her for complaining about sexual harassment by Mr. Ailes. The suit by Ms. Tantaros, filed in New York State Supreme Court in Manhattan, is the latest round in a contentious volley that began in late winter, when Fox claimed she had breached her employment contract by writing a book without receiving network approval. â€œFox News masquerades as a defender of traditional family values, but behind the scenes, it operates like a   Playboy   cult, steeped in intimidation, indecency and misogyny,â€ Ms. Tantarosâ€™s suit says. Fox News said it would not comment on pending litigation. Mr. Ailes, the networkâ€™s founding chairman and guiding force for two decades, resigned last month after a former anchor, Gretchen Carlson, said in a suit that she was fired for refusing his sexual advances. Mr. Ailes has denied all allegations of harassment. In April, the chief lawyer for Fox charged that Ms. Tantaros had concocted sexual harassment claims to gain leverage in the contract dispute her lawyer, Judd Burstein, said the book dispute was a pretext that Fox was using to silence her. During arbitration, Mr. Burstein said, Fox News offered to pay her a sum â€œin the seven figuresâ€ if she renounced claims against Mr. Ailes and others at the network, including the host Bill Oâ€™Reilly. According to the lawsuit, Ms. Tantaros said she had been subjected to unwelcome advances from Mr. Oâ€™Reilly, whom she had regarded as a friend and adviser. â€œAiles did not act alone,â€ the lawsuit states. â€œHe may have been the primary culprit, but his actions were condoned by his most senior lieutenants, who engaged in a concerted effort to silence Tantaros by threats, humiliation and retaliation. â€ Ms. Tantaros also claimed in the lawsuit that she was the subject of humiliating posts by pseudonymous accounts on Twitter known as â€œsock puppetsâ€ that she says were instigated by the Fox News publicity department. Ms. Tantaros joined Fox as a contributor in 2010, and a year later was named   of â€œThe Five,â€ which aired at 5 p. m. She said in the suit that she was repeatedly told by Fox executives that she could not wear pants on the air because â€œRoger wants to see your legs. â€ The lawsuit goes on to say that on Aug. 12, 2014, Mr. Ailes called her into his office and asked if she was planning to marry and have children. â€œAiles then started complaining about marriage in general, and also made   jokes about being married,â€ the lawsuit states. It describes Mr. Ailes as speculating on the sexual habits and preferences of 10 Fox News personalities. He asked Ms. Tantaros to turn around â€œso I can get a good look at you,â€ the lawsuit charges, adding that Ms. Tantaros refused. Soon after, she was moved from â€œThe Fiveâ€ to a   show, â€œOutnumbered,â€ that aired at midday. Mr. Ailes called her back for similar sessions in December 2014 and February 2015, the lawsuit charges, and when she continued to rebuff him, she encountered hostility from the Fox News publicity department. In the February meeting, she said, Mr. Ailes talked about how she would look in a bikini, and accused her of ending a   relationship because she had been merely using the man. The episode brought her to tears, the lawsuit states. She said the sole interview arranged by the publicity department during that period was with a writer for a blog controlled by Fox, who asked about her breasts and if she was difficult to work with. In April 2015, the lawsuit states, Ms. Tantaros met with Bill Shine, then a senior news executive and close aide to Mr. Ailes. She said that she told him about the meetings with Mr. Ailes and asked if he had told the head of publicity for Fox News, Irena Briganti, to go after her. The lawsuit claims that Mr. Shine â€œtold Tantaros that Briganti is like a rabid dog on a chain that we canâ€™t control. Sometimes that dog gets off the chain. â€ Then, pointing to a picture of Mr. Ailes on a magazine cover, the lawsuit charges, Mr. Shine told her that â€œthis powerful man has faith in Irena Brigantiâ€ and that Ms. Tantaros â€œneeds to let this one go. â€ Mr. Shine, through a spokeswoman, has said that Ms. Tantaros never approached him about Mr. Ailes harassing her. Mr. Shine was named   of Fox News after Mr. Ailes departed.\"\"\")\n",
    "if(ans == 0):\n",
    "    print(\"FALSE NEWS!!!\")\n",
    "else:\n",
    "    print(\"TRUE NEWS\")\n",
    "# false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = fake_news(\"\"\"House Dem Aide: We Didnâ€™t Even See Comeyâ€™s Letter Until Jason Chaffetz Tweeted It By Darrell Lucus on October 30, 2016 Subscribe Jason Chaffetz on the stump in American Fork, Utah ( image courtesy Michael Jolley, available under a Creative Commons-BY license) \n",
    "With apologies to Keith Olbermann, there is no doubt who the Worst Person in The World is this weekâ€“FBI Director James Comey. But according to a House Democratic aide, it looks like we also know who the second-worst person is as well. It turns out that when Comey sent his now-infamous letter announcing that the FBI was looking into emails that may be related to Hillary Clintonâ€™s email server, the ranking Democrats on the relevant committees didnâ€™t hear about it from Comey. They found out via a tweet from one of the Republican committee chairmen. \n",
    "As we now know, Comey notified the Republican chairmen and Democratic ranking members of the House Intelligence, Judiciary, and Oversight committees that his agency was reviewing emails it had recently discovered in order to see if they contained classified information. Not long after this letter went out, Oversight Committee Chairman Jason Chaffetz set the political world ablaze with this tweet. FBI Dir just informed me, \"The FBI has learned of the existence of emails that appear to be pertinent to the investigation.\" Case reopened \n",
    "â€” Jason Chaffetz (@jasoninthehouse) October 28, 2016 \n",
    "Of course, we now know that this was not the case . Comey was actually saying that it was reviewing the emails in light of â€œan unrelated caseâ€â€“which we now know to be Anthony Weinerâ€™s sexting with a teenager. But apparently such little things as facts didnâ€™t matter to Chaffetz. The Utah Republican had already vowed to initiate a raft of investigations if Hillary winsâ€“at least two yearsâ€™ worth, and possibly an entire termâ€™s worth of them. Apparently Chaffetz thought the FBI was already doing his work for himâ€“resulting in a tweet that briefly roiled the nation before cooler heads realized it was a dud. \n",
    "But according to a senior House Democratic aide, misreading that letter may have been the least of Chaffetzâ€™ sins. That aide told Shareblue that his boss and other Democrats didnâ€™t even know about Comeyâ€™s letter at the timeâ€“and only found out when they checked Twitter. â€œDemocratic Ranking Members on the relevant committees didnâ€™t receive Comeyâ€™s letter until after the Republican Chairmen. In fact, the Democratic Ranking Members didnâ€™ receive it until after the Chairman of the Oversight and Government Reform Committee, Jason Chaffetz, tweeted it out and made it public.â€ \n",
    "So letâ€™s see if weâ€™ve got this right. The FBI director tells Chaffetz and other GOP committee chairmen about a major development in a potentially politically explosive investigation, and neither Chaffetz nor his other colleagues had the courtesy to let their Democratic counterparts know about it. Instead, according to this aide, he made them find out about it on Twitter. \n",
    "There has already been talk on Daily Kos that Comey himself provided advance notice of this letter to Chaffetz and other Republicans, giving them time to turn on the spin machine. That may make for good theater, but there is nothing so far that even suggests this is the case. After all, there is nothing so far that suggests that Comey was anything other than grossly incompetent and tone-deaf. \n",
    "What it does suggest, however, is that Chaffetz is acting in a way that makes Dan Burton and Darrell Issa look like models of responsibility and bipartisanship. He didnâ€™t even have the decency to notify ranking member Elijah Cummings about something this explosive. If that doesnâ€™t trample on basic standards of fairness, I donâ€™t know what does. \n",
    "Granted, itâ€™s not likely that Chaffetz will have to answer for this. He sits in a ridiculously Republican district anchored in Provo and Orem; it has a Cook Partisan Voting Index of R+25, and gave Mitt Romney a punishing 78 percent of the vote in 2012. Moreover, the Republican House leadership has given its full support to Chaffetzâ€™ planned fishing expedition. But that doesnâ€™t mean we canâ€™t turn the hot lights on him. After all, he is a textbook example of what the House has become under Republican control. And he is also the Second Worst Person in the World. About Darrell Lucus \n",
    "Darrell is a 30-something graduate of the University of North Carolina who considers himself a journalist of the old school. An attempt to turn him into a member of the religious right in college only succeeded in turning him into the religious right's worst nightmare--a charismatic Christian who is an unapologetic liberal. His desire to stand up for those who have been scared into silence only increased when he survived an abusive three-year marriage. You may know him on Daily Kos as Christian Dem in NC . Follow him on Twitter @DarrellLucus or connect with him on Facebook . Click here to buy Darrell a Mello Yello. Connect\"\"\")\n",
    "if(ans == 0):\n",
    "    print(\"FALSE NEWS!!!\")\n",
    "else:\n",
    "    print(\"TRUE NEWS\")\n",
    "# true"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
